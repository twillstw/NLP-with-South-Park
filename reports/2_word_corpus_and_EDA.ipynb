{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Word Corpus and EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the dataset cleaned and truncated, this notebook dives deeper into the actual dialogue and building the word corpus. A large part of that involves examining stop words and making decisions on which words to drop from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few cells set up the data. For more details refer to notebook \\#1. The adjusted data set is named `lines_final`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = pd.read_csv('../data/All-seasons.csv')\n",
    "\n",
    "lines = lines[lines.Season != 'Season']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines[['Season', 'Episode']] = lines[['Season', 'Episode']].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_chars = ['Mr. Garrison', 'Chef', 'Kenny', 'Sharon',\\\n",
    "                 'Mr. Mackey', 'Gerald', 'Liane', 'Sheila',\\\n",
    "                 'Stephen', 'Ms. Garrison', 'Mrs. Garrison']\n",
    "\n",
    "lines.loc[lines.Character.isin(support_chars), 'Character'] = 'Support Character'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_labels = ['Cartman', 'Stan', 'Kyle', 'Butters', 'Randy', 'Support Character']\n",
    "\n",
    "lines_final = lines[lines.Character.isin(final_labels)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the basic corpus is created. Essentially, all the strings of dialogue from the `Line` column are compiled into one list. Then, regular expressions are used to remove the new line figure `\\n` from each string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You guys, you guys! Chef is going away.',\n",
       " 'Going away? For how long?',\n",
       " 'Forever.',\n",
       " \"I'm sorry boys.\",\n",
       " \"Chef said he's been bored, so he joining a group called the Super Adventure Club.\",\n",
       " 'Wow!',\n",
       " 'Chef?? What kind of questions do you think adventuring around the world is gonna answer?!',\n",
       " \"What's the meaning of life? Why are we here?\",\n",
       " \"I hope you're making the right choice.\",\n",
       " \"I'm gonna miss him.  I'm gonna miss Chef and I...and I don't know how to tell him!\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "corpus = lines_final.Line.tolist()\n",
    "\n",
    "for line in range(len(corpus)):\n",
    "    corpus[line] = re.sub('\\\\n', '', corpus[line].rstrip())\n",
    "    \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Average length</b><br>\n",
    "What is the average document length for each line?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.004596119444063"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(doc.split()) for doc in corpus])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the corpus created, we can now get even more granular and examine the individual words. The words from the corpus are compiled into one list and then converted to lowercase with all punctuation removed. Then a counter is called and all the words are added to an OrderedDict according to their frequency within the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "words = \" \".join(corpus).lower()\n",
    "words = \" \".join(word.strip(string.punctuation) for word in words.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "\n",
    "word_counts = Counter(words.split())\n",
    "\n",
    "token_dict = OrderedDict(word_counts.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the OrderedDict shows which words show up the most, indicating likely stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('you', 13092),\n",
       " ('the', 10677),\n",
       " ('i', 10013),\n",
       " ('to', 9533),\n",
       " ('a', 7179),\n",
       " ('and', 6167),\n",
       " ('it', 5443),\n",
       " ('that', 4642),\n",
       " ('we', 4595),\n",
       " ('is', 4454),\n",
       " ('what', 4051),\n",
       " ('of', 4035),\n",
       " ('this', 3401),\n",
       " ('in', 3324),\n",
       " ('have', 3149),\n",
       " ('my', 3101),\n",
       " ('on', 3082),\n",
       " ('oh', 3077),\n",
       " ('all', 2946),\n",
       " ('just', 2875),\n",
       " ('do', 2770),\n",
       " ('me', 2727),\n",
       " (\"i'm\", 2707),\n",
       " ('no', 2584),\n",
       " ('for', 2562),\n",
       " (\"don't\", 2545),\n",
       " ('are', 2467),\n",
       " ('be', 2301),\n",
       " ('your', 2276),\n",
       " (\"it's\", 2235),\n",
       " ('yeah', 2199),\n",
       " ('get', 2195),\n",
       " ('but', 2000),\n",
       " ('not', 1972),\n",
       " ('with', 1960),\n",
       " ('know', 1948),\n",
       " ('so', 1925),\n",
       " ('dude', 1915),\n",
       " ('now', 1894),\n",
       " ('well', 1886),\n",
       " ('go', 1836),\n",
       " ('can', 1770),\n",
       " ('right', 1710),\n",
       " ('out', 1673),\n",
       " ('like', 1621),\n",
       " ('was', 1610),\n",
       " ('gonna', 1595),\n",
       " (\"that's\", 1565),\n",
       " ('here', 1564),\n",
       " ('guys', 1511)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k, v) for k, v in token_dict.items()][:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond just identifying common words, below are a couple functions to analyze different types of word frequency. The first function compares the total number of times a word appears in the corpus, total frequency, versus the number of documents containing the word, the document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_compare(word, corpus=corpus):\n",
    "    '''Takes a word and a word corpus and calculates\n",
    "    the total word frequency and the number of documents\n",
    "    containing that word.'''\n",
    "    \n",
    "    word = word.lower()\n",
    "    \n",
    "    words = \" \".join(corpus).lower()\n",
    "    words = \" \".join(word.strip(string.punctuation) for word in words.split())\n",
    "    \n",
    "    word_counts = Counter(words.split())\n",
    "    token_dict = OrderedDict(word_counts.most_common())\n",
    "    \n",
    "    total_frequency = token_dict[word]\n",
    "    \n",
    "    doc_freq = 0\n",
    "    \n",
    "    for line in corpus:\n",
    "        if word in [token.strip(string.punctuation).lower() for token in line.split()]:\n",
    "            doc_freq += 1\n",
    "    \n",
    "    print('The total frequency of the word \\'{}\\' is: \\t'.format(word), total_frequency)\n",
    "    print('The number of documents with the word \\'{}\\': \\t'.format(word), doc_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total frequency of the word 'a' is: \t 3\n",
      "The number of documents with the word 'a': \t 2\n"
     ]
    }
   ],
   "source": [
    "freq_compare('A', ['a', 'a a.', 'b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function compares the total frequency of a particular word between the different character labels. This is useful for assessing if a particular word has more importance for specific characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_labels(term):\n",
    "    '''Takes a particular word and calculates\n",
    "    how often it is used by each character'''\n",
    "    \n",
    "    term = term.lower()\n",
    "    \n",
    "    count_dict = {'Cartman': 0, 'Stan': 0, 'Kyle': 0, 'Butters': 0,\\\n",
    "                  'Randy': 0, 'Support Character': 0}\n",
    "    \n",
    "    for k, v in count_dict.items():\n",
    "        subset = lines_final[lines_final.Character == k]\n",
    "        \n",
    "        corpus = subset.Line.tolist()\n",
    "\n",
    "        for line in range(len(corpus)):\n",
    "            corpus[line] = re.sub('\\\\n', '', corpus[line].rstrip())\n",
    "        \n",
    "        words = \" \".join(corpus).lower()\n",
    "        words = \" \".join(word.strip(string.punctuation) for word in words.split())\n",
    "    \n",
    "        word_counts = Counter(words.split())\n",
    "        token_dict = OrderedDict(word_counts.most_common())\n",
    "        \n",
    "        if term in token_dict.keys():\n",
    "            count_dict[k] += token_dict[term]\n",
    "    \n",
    "    print('How often the word \\'{}\\' appears in each class:'.format(term))\n",
    "    print(count_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Trying the functions: </b>Now that the functions are defined, let's examine a couple words from `token_dict`, the OrderedDict of all words. It helps to keep in mind that there are approximately 36,000 documents in the data, and that Cartman, Stan and Kyle have the majority of the lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total frequency of the word 'the' is: \t 10677\n",
      "The number of documents with the word 'the': \t 7654\n",
      "How often the word 'the' appears in each class:\n",
      "{'Cartman': 3678, 'Stan': 1729, 'Kyle': 1565, 'Butters': 683, 'Randy': 909, 'Support Character': 2113}\n"
     ]
    }
   ],
   "source": [
    "freq_compare('The')\n",
    "compare_labels('The')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total frequency of the word 'but' is: \t 2000\n",
      "The number of documents with the word 'but': \t 1858\n",
      "How often the word 'but' appears in each class:\n",
      "{'Cartman': 546, 'Stan': 366, 'Kyle': 342, 'Butters': 217, 'Randy': 168, 'Support Character': 361}\n"
     ]
    }
   ],
   "source": [
    "freq_compare('but')\n",
    "compare_labels('but')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total frequency of the word 'dude' is: \t 1915\n",
      "The number of documents with the word 'dude': \t 1867\n",
      "How often the word 'dude' appears in each class:\n",
      "{'Cartman': 415, 'Stan': 853, 'Kyle': 607, 'Butters': 0, 'Randy': 3, 'Support Character': 37}\n"
     ]
    }
   ],
   "source": [
    "freq_compare('dude')\n",
    "compare_labels('dude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words 'the' and 'but' are common words and have fairly uniform frequencies. On the other hand, the word 'dude', although nearly as frequent as 'but', is used much more frequently by the first three characters. This shows that it might be a good word to help identify those characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_southpark",
   "language": "python",
   "name": "nlp_southpark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
