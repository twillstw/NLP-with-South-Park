{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Word Corpus and EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the dataset cleaned and truncated, this notebook dives deeper into the actual dialogue and building the word corpus. A large part of that involves examining stop words and making decisions on which words to drop from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few cells set up the data. For more details refer to notebook \\#1. The adjusted data set is named `lines_final`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = pd.read_csv('../data/All-seasons.csv')\n",
    "\n",
    "lines = lines[lines.Season != 'Season']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines[['Season', 'Episode']] = lines[['Season', 'Episode']].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_chars = ['Mr. Garrison', 'Chef', 'Sharon',\\\n",
    "                 'Mr. Mackey', 'Gerald', 'Liane', 'Sheila',\\\n",
    "                 'Stephen', 'Ms. Garrison', 'Mrs. Garrison']\n",
    "\n",
    "lines.loc[lines.Character.isin(support_chars), 'Character'] = 'Support Character'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_labels = ['Cartman', 'Stan', 'Kyle', 'Butters', 'Randy', 'Support Character']\n",
    "\n",
    "lines_final = lines[lines.Character.isin(final_labels)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the basic corpus is created. Essentially, all the strings of dialogue from the `Line` column are compiled into one list. Then, regular expressions are used to remove the new line figure `\\n` from each string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You guys, you guys! Chef is going away.',\n",
       " 'Going away? For how long?',\n",
       " 'Forever.',\n",
       " \"I'm sorry boys.\",\n",
       " \"Chef said he's been bored, so he joining a group called the Super Adventure Club.\",\n",
       " 'Wow!',\n",
       " 'Chef?? What kind of questions do you think adventuring around the world is gonna answer?!',\n",
       " \"What's the meaning of life? Why are we here?\",\n",
       " \"I hope you're making the right choice.\",\n",
       " \"I'm gonna miss him.  I'm gonna miss Chef and I...and I don't know how to tell him!\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "corpus = lines_final.Line.tolist()\n",
    "\n",
    "for line in range(len(corpus)):\n",
    "    corpus[line] = re.sub('\\\\n', '', corpus[line].rstrip())\n",
    "    \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Average length</b><br>\n",
    "What is the average document length for each line?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.160658881931518"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(doc.split()) for doc in corpus])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the corpus created, we can now get even more granular and examine the individual words. First, the contractions are removed. Then, the words from the corpus are compiled into one list and then converted to lowercase with all punctuation removed. Next, the verbs and nouns are lemmatized. And finally, a counter is called and all the words are added to an OrderedDict according to their frequency within the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import contractions\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "for line in range(len(corpus)):\n",
    "    corpus[line] = contractions.fix(corpus[line])\n",
    "\n",
    "words = \" \".join(corpus).lower()\n",
    "words = \" \".join(word.strip(string.punctuation) for word in words.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()\n",
    "\n",
    "word_list = word_tokenize(words)\n",
    "word_list = [lem.lemmatize(w, pos='v') for w in word_list]\n",
    "words = ' '.join([lem.lemmatize(w) for w in word_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "\n",
    "word_counts = Counter(words.split())\n",
    "\n",
    "token_dict = OrderedDict(word_counts.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the OrderedDict shows which words show up the most, indicating likely stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('be', 25112),\n",
       " ('you', 14922),\n",
       " ('i', 13788),\n",
       " ('to', 12042),\n",
       " ('the', 10594),\n",
       " ('do', 8179),\n",
       " ('not', 7950),\n",
       " ('it', 7724),\n",
       " ('a', 7723),\n",
       " ('we', 6360),\n",
       " ('that', 6171),\n",
       " ('and', 6119),\n",
       " ('have', 5864),\n",
       " ('go', 4624),\n",
       " ('what', 4579),\n",
       " ('get', 4073),\n",
       " ('of', 4018),\n",
       " ('this', 3368),\n",
       " ('in', 3308),\n",
       " ('my', 3067),\n",
       " ('on', 3053),\n",
       " ('oh', 3025),\n",
       " ('all', 2933),\n",
       " ('can', 2923),\n",
       " ('just', 2859),\n",
       " ('me', 2697),\n",
       " ('for', 2547),\n",
       " ('no', 2534),\n",
       " ('your', 2265),\n",
       " ('he', 2196),\n",
       " ('know', 2125),\n",
       " ('will', 2100),\n",
       " ('yeah', 2097),\n",
       " ('but', 1994),\n",
       " ('with', 1948),\n",
       " ('so', 1915),\n",
       " ('they', 1908),\n",
       " ('dude', 1904),\n",
       " ('now', 1878),\n",
       " ('well', 1874),\n",
       " (\"'s\", 1858),\n",
       " ('guy', 1812),\n",
       " ('u', 1806),\n",
       " ('come', 1777),\n",
       " ('right', 1707),\n",
       " ('like', 1665),\n",
       " ('out', 1663),\n",
       " ('want', 1641),\n",
       " ('here', 1624),\n",
       " ('there', 1597),\n",
       " ('think', 1589),\n",
       " ('kyle', 1582),\n",
       " ('up', 1471),\n",
       " ('about', 1450),\n",
       " ('see', 1432),\n",
       " ('how', 1319),\n",
       " ('say', 1287),\n",
       " ('okay', 1279),\n",
       " ('uh', 1251),\n",
       " ('our', 1240),\n",
       " ('make', 1226),\n",
       " ('if', 1212),\n",
       " ('look', 1197),\n",
       " ('hey', 1196),\n",
       " ('at', 1165),\n",
       " ('let', 1155),\n",
       " ('cartman', 1098),\n",
       " ('people', 1055),\n",
       " ('take', 1019),\n",
       " ('really', 999),\n",
       " ('tell', 972),\n",
       " ('would', 972),\n",
       " ('time', 967),\n",
       " ('butter', 939),\n",
       " ('why', 920),\n",
       " ('one', 909),\n",
       " ('stan', 889),\n",
       " ('god', 887),\n",
       " ('who', 884),\n",
       " ('kenny', 874),\n",
       " ('then', 873),\n",
       " ('him', 860),\n",
       " ('back', 853),\n",
       " ('because', 848),\n",
       " ('need', 831),\n",
       " ('from', 742),\n",
       " ('some', 738),\n",
       " ('mr', 729),\n",
       " ('them', 726),\n",
       " ('good', 718),\n",
       " ('his', 714),\n",
       " ('little', 713),\n",
       " ('thing', 699),\n",
       " ('mean', 682),\n",
       " ('mom', 675),\n",
       " ('yes', 663),\n",
       " ('dad', 653),\n",
       " ('where', 638),\n",
       " ('could', 628),\n",
       " ('over', 617),\n",
       " ('boy', 614),\n",
       " ('try', 611),\n",
       " ('hell', 602),\n",
       " ('wait', 600),\n",
       " ('something', 597),\n",
       " ('she', 590),\n",
       " ('give', 585),\n",
       " ('friend', 574),\n",
       " ('should', 572),\n",
       " ('way', 565),\n",
       " ('kid', 564),\n",
       " ('too', 560),\n",
       " ('an', 560),\n",
       " ('when', 558),\n",
       " ('or', 525),\n",
       " ('sure', 515),\n",
       " ('down', 515),\n",
       " ('child', 512),\n",
       " ('talk', 509),\n",
       " ('stop', 508),\n",
       " ('play', 489),\n",
       " ('off', 488),\n",
       " ('huh', 486),\n",
       " ('more', 486),\n",
       " ('thank', 485),\n",
       " ('eric', 475),\n",
       " ('find', 469),\n",
       " ('school', 469),\n",
       " ('alright', 456),\n",
       " ('help', 444),\n",
       " ('man', 439),\n",
       " ('please', 435),\n",
       " ('stupid', 433),\n",
       " ('call', 432),\n",
       " ('her', 430),\n",
       " ('show', 428),\n",
       " ('again', 424),\n",
       " ('work', 419),\n",
       " ('maybe', 417),\n",
       " ('sorry', 409),\n",
       " ('everyone', 406),\n",
       " ('by', 403),\n",
       " ('kill', 400),\n",
       " ('never', 398),\n",
       " ('jesus', 397),\n",
       " ('cool', 397),\n",
       " ('put', 397),\n",
       " ('their', 396),\n",
       " ('fuck', 391),\n",
       " ('even', 383),\n",
       " ('those', 382),\n",
       " ('only', 381),\n",
       " ('love', 380),\n",
       " ('day', 378),\n",
       " ('better', 372),\n",
       " ('new', 371),\n",
       " ('these', 367),\n",
       " ('money', 360),\n",
       " ('feel', 359),\n",
       " ('any', 355),\n",
       " ('start', 355),\n",
       " ('as', 344),\n",
       " ('into', 344),\n",
       " ('watch', 342),\n",
       " ('away', 337),\n",
       " ('happen', 337),\n",
       " ('very', 336),\n",
       " ('big', 336),\n",
       " ('live', 333),\n",
       " ('leave', 332),\n",
       " ('two', 330),\n",
       " ('hello', 330),\n",
       " ('much', 325),\n",
       " ('ah', 324),\n",
       " ('believe', 323),\n",
       " ('still', 319),\n",
       " ('son', 318),\n",
       " ('hear', 318),\n",
       " ('bad', 318),\n",
       " ('keep', 314),\n",
       " ('home', 313),\n",
       " ('guess', 309),\n",
       " ('anything', 307),\n",
       " ('great', 305),\n",
       " ('ever', 301),\n",
       " ('use', 301),\n",
       " ('than', 299),\n",
       " (\"m'kay\", 297),\n",
       " ('wow', 295),\n",
       " ('other', 288),\n",
       " ('crap', 284),\n",
       " ('nothing', 283),\n",
       " ('care', 282),\n",
       " ('aw', 280),\n",
       " ('suck', 280),\n",
       " ('chef', 277),\n",
       " ('around', 274),\n",
       " ('fine', 269),\n",
       " ('parent', 268),\n",
       " ('nice', 267),\n",
       " ('first', 262),\n",
       " ('bitch', 261),\n",
       " ('lot', 257),\n",
       " ('house', 257),\n",
       " ('die', 255),\n",
       " ('kind', 253),\n",
       " ('wrong', 253),\n",
       " ('girl', 252),\n",
       " ('real', 244),\n",
       " ('night', 240),\n",
       " ('game', 239),\n",
       " ('ball', 238),\n",
       " ('ike', 237),\n",
       " ('wendy', 237),\n",
       " ('whoa', 237),\n",
       " ('everything', 236),\n",
       " ('understand', 235),\n",
       " ('pretty', 234),\n",
       " ('life', 233),\n",
       " ('before', 232),\n",
       " ('last', 232),\n",
       " ('must', 231),\n",
       " ('stuff', 231),\n",
       " ('best', 231),\n",
       " ('long', 229),\n",
       " ('eat', 225),\n",
       " ('idea', 224),\n",
       " ('dollar', 223),\n",
       " ('minute', 222),\n",
       " ('else', 221),\n",
       " ('hold', 221),\n",
       " ('world', 220),\n",
       " ('jew', 219),\n",
       " ('family', 218),\n",
       " ('learn', 218),\n",
       " ('whole', 213),\n",
       " ('listen', 213),\n",
       " ('park', 211),\n",
       " ('year', 211),\n",
       " ('bring', 211),\n",
       " ('damn', 210),\n",
       " ('stay', 209),\n",
       " ('suppose', 209),\n",
       " ('asshole', 208),\n",
       " ('fat', 208),\n",
       " ('change', 206),\n",
       " ('today', 204),\n",
       " ('stanley', 204),\n",
       " ('gay', 200),\n",
       " ('shit', 200)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k, v) for k, v in token_dict.items()][:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond just identifying common words, below are a couple functions to analyze different types of word frequency. The first function compares the total number of times a word appears in the corpus, total frequency, versus the number of documents containing the word, the document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_compare(word, corpus=corpus):\n",
    "    '''Takes a word and a word corpus and calculates\n",
    "    the total word frequency and the number of documents\n",
    "    containing that word.'''\n",
    "    \n",
    "    word = word.lower()\n",
    "    \n",
    "    words = \" \".join(corpus).lower()\n",
    "    words = \" \".join(word.strip(string.punctuation) for word in words.split())\n",
    "    \n",
    "    word_counts = Counter(words.split())\n",
    "    token_dict = OrderedDict(word_counts.most_common())\n",
    "    \n",
    "    total_frequency = token_dict[word]\n",
    "    \n",
    "    doc_freq = 0\n",
    "    \n",
    "    for line in corpus:\n",
    "        if word in [token.strip(string.punctuation).lower() for token in line.split()]:\n",
    "            doc_freq += 1\n",
    "    \n",
    "    print('The total frequency of the word \\'{}\\' is: \\t'.format(word), total_frequency)\n",
    "    print('The number of documents with the word \\'{}\\': \\t'.format(word), doc_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total frequency of the word 'a' is: \t 3\n",
      "The number of documents with the word 'a': \t 2\n"
     ]
    }
   ],
   "source": [
    "freq_compare('A', ['a', 'a a.', 'b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function compares the total frequency of a particular word between the different character labels. This is useful for assessing if a particular word has more importance for specific characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_labels(term):\n",
    "    '''Takes a particular word and calculates\n",
    "    how often it is used by each character'''\n",
    "    \n",
    "    term = term.lower()\n",
    "    \n",
    "    count_dict = {'Cartman': 0, 'Stan': 0, 'Kyle': 0, 'Butters': 0,\\\n",
    "                  'Randy': 0, 'Support Character': 0}\n",
    "    \n",
    "    for k, v in count_dict.items():\n",
    "        subset = lines_final[lines_final.Character == k]\n",
    "        \n",
    "        corpus = subset.Line.tolist()\n",
    "\n",
    "        for line in range(len(corpus)):\n",
    "            corpus[line] = re.sub('\\\\n', '', corpus[line].rstrip())\n",
    "        \n",
    "        words = \" \".join(corpus).lower()\n",
    "        words = \" \".join(word.strip(string.punctuation) for word in words.split())\n",
    "    \n",
    "        word_counts = Counter(words.split())\n",
    "        token_dict = OrderedDict(word_counts.most_common())\n",
    "        \n",
    "        if term in token_dict.keys():\n",
    "            count_dict[k] += token_dict[term]\n",
    "        \n",
    "        # Now convert to a ratio\n",
    "        count_dict[k] = round((count_dict[k] / len(subset)), 3)\n",
    "    \n",
    "    print('How often the word \\'{}\\' appears in each class:'.format(term))\n",
    "    print(count_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Trying the functions: </b><br>\n",
    "Now that the functions are defined, let's examine a couple words from `token_dict`, the OrderedDict of all words. It helps to keep in mind that there are approximately 36,000 documents in the data, and that Cartman, Stan and Kyle have the majority of the lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total frequency of the word 'the' is: \t 10585\n",
      "The number of documents with the word 'the': \t 7587\n",
      "How often the word 'the' appears in each class:\n",
      "{'Cartman': 0.376, 'Stan': 0.225, 'Kyle': 0.22, 'Butters': 0.262, 'Randy': 0.368, 'Support Character': 0.343}\n"
     ]
    }
   ],
   "source": [
    "freq_compare('The')\n",
    "compare_labels('The')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total frequency of the word 'but' is: \t 1989\n",
      "The number of documents with the word 'but': \t 1847\n",
      "How often the word 'but' appears in each class:\n",
      "{'Cartman': 0.056, 'Stan': 0.048, 'Kyle': 0.048, 'Butters': 0.083, 'Randy': 0.068, 'Support Character': 0.06}\n"
     ]
    }
   ],
   "source": [
    "freq_compare('but')\n",
    "compare_labels('but')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total frequency of the word 'dude' is: \t 1881\n",
      "The number of documents with the word 'dude': \t 1833\n",
      "How often the word 'dude' appears in each class:\n",
      "{'Cartman': 0.042, 'Stan': 0.111, 'Kyle': 0.086, 'Butters': 0.0, 'Randy': 0.001, 'Support Character': 0.001}\n"
     ]
    }
   ],
   "source": [
    "freq_compare('dude')\n",
    "compare_labels('dude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words 'the' and 'but' are common words and have fairly uniform frequencies. On the other hand, the word 'dude', although nearly as frequent as 'but', is used much more frequently by the first three characters. This shows that it might be a good word to help identify those characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Sifting through stopwords</b><br>\n",
    "With the functions defined, we can now run them over the list of common words to check the frequency comparisons and confirm which ones should be dropped as stopwords. Below, I have defined a small function to combine the two functions from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_freqs(word):\n",
    "    freq_compare(word)\n",
    "    compare_labels(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total frequency of the word 'than' is: \t 299\n",
      "The number of documents with the word 'than': \t 286\n",
      "How often the word 'than' appears in each class:\n",
      "{'Cartman': 0.01, 'Stan': 0.008, 'Kyle': 0.008, 'Butters': 0.007, 'Randy': 0.009, 'Support Character': 0.007}\n"
     ]
    }
   ],
   "source": [
    "word_freqs(\"than\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this process, I chose to manually check each word in order to really dig into the list and compare each one. Having done so, the final list of stopwords is defined below. While none of the words in the list are too surprising, some of the words I chose to not treat as stopwords may be of interest. For example, I chose to keep 'my' and 'me' because they had slightly inbalanced distributions, which might owe to Cartman's selfish nature. I also chose to keep 'no' and 'not' because those might be useful later when moving beyond a simple bag-of-words model.<br>\n",
    "<br>\n",
    "The final list contains 53 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['be', 'you', 'i', 'to', 'the', 'do', 'it',\\\n",
    "              'a', 'we', 'that', 'and', 'have', 'go', 'what',\\\n",
    "              'get', 'of', 'this', 'in', 'on', 'all', 'just',\\\n",
    "              'for', 'he', 'know', 'will', 'but', 'with', 'so',\\\n",
    "              'they', 'now', 'well', \"'s\", 'guy', 'u', 'come',\\\n",
    "              'like', 'there', 'at', 'would', 'who', 'him',\\\n",
    "              'them', 'his', 'thing', 'where', 'should', 'an',\\\n",
    "              'please', 'maybe', 'their', 'even', 'any', 'than']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing I noticed while testing the stopwords is that the frequencies for Kyle and Stan were consistenly lower than the other characters. I don't know the exact reason for this, but it seems to indicate that the dialogue for those two characters is a bit more unique.<br>\n",
    "Here are a couple examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How often the word 'in' appears in each class:\n",
      "{'Cartman': 0.116, 'Stan': 0.07, 'Kyle': 0.067, 'Butters': 0.086, 'Randy': 0.114, 'Support Character': 0.112}\n"
     ]
    }
   ],
   "source": [
    "compare_labels('in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How often the word 'will' appears in each class:\n",
      "{'Cartman': 0.027, 'Stan': 0.013, 'Kyle': 0.014, 'Butters': 0.02, 'Randy': 0.021, 'Support Character': 0.025}\n"
     ]
    }
   ],
   "source": [
    "compare_labels('will')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequency proportions are lowere for Kyle and Stan. This type of pattern was surprisingly consistent throughout the list of stopwords"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_southpark",
   "language": "python",
   "name": "nlp_southpark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
