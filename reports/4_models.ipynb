{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now to try and predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to set up the data again. As before, refer to notebooks 1 and 2 for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the data\n",
    "lines = pd.read_csv('../data/All-seasons.csv')\n",
    "\n",
    "lines = lines[lines.Season != 'Season']\n",
    "\n",
    "lines[['Season', 'Episode']] = lines[['Season', 'Episode']].astype('int64')\n",
    "\n",
    "support_chars = ['Mr. Garrison', 'Chef', 'Sharon',\\\n",
    "                 'Mr. Mackey', 'Gerald', 'Liane', 'Sheila',\\\n",
    "                 'Stephen', 'Ms. Garrison', 'Mrs. Garrison']\n",
    "\n",
    "lines.loc[lines.Character.isin(support_chars), 'Character'] = 'Support Character'\n",
    "\n",
    "final_labels = ['Cartman', 'Stan', 'Kyle', 'Butters', 'Randy', 'Support Character']\n",
    "\n",
    "lines_final = lines[lines.Character.isin(final_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Character</th>\n",
       "      <th>Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Stan</td>\n",
       "      <td>You guys, you guys! Chef is going away. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Kyle</td>\n",
       "      <td>Going away? For how long?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Stan</td>\n",
       "      <td>Forever.\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Season  Episode Character                                        Line\n",
       "0      10        1      Stan  You guys, you guys! Chef is going away. \\n\n",
       "1      10        1      Kyle                 Going away? For how long?\\n\n",
       "2      10        1      Stan                                  Forever.\\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_final.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus is established here, with steps to convert everything to lowercase and remove punctuation from the end of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you guys you guys chef is going away', 'going away for how long', 'forever']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, string\n",
    "\n",
    "corpus = lines_final.Line.tolist()\n",
    "\n",
    "for line in range(len(corpus)):\n",
    "    corpus[line] = re.sub('\\\\n', '', corpus[line].rstrip()).lower()\n",
    "    corpus[line] = \" \".join(word.strip(string.punctuation) for word in corpus[line].split())\n",
    "    \n",
    "corpus[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Improving the corpus by removing contractions and lemmatizing words</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a loop to expand all contractions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what is the meaning of life why are we here',\n",
       " 'i hope you are making the right choice',\n",
       " 'I am going to miss him I am going to miss chef and i...and i do not know how to tell him']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for line in range(len(corpus)):\n",
    "    corpus[line] = contractions.fix(corpus[line])\n",
    "                                  \n",
    "corpus[7:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a function to lemmatize all verbs and nouns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_lines(line):\n",
    "    word_list = word_tokenize(line)\n",
    "    \n",
    "    word_list = [lem.lemmatize(w, pos='v') for w in word_list]\n",
    "    \n",
    "    lem_line = ' '.join([lem.lemmatize(w) for w in word_list])\n",
    "    \n",
    "    return lem_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, a loop using the previous function to execute the lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what be the mean of life why be we here',\n",
       " 'i hope you be make the right choice']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for line in range(len(corpus)):\n",
    "    corpus[line] = lemmatize_lines(corpus[line])\n",
    "    \n",
    "corpus[7:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of stop words as determined in notebook 2 needs to instantiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = ['be', 'you', 'i', 'to', 'the', 'do', 'it',\\\n",
    "        'a', 'we', 'that', 'and', 'have', 'go', 'what',\\\n",
    "        'get', 'of', 'this', 'in', 'on', 'all', 'just',\\\n",
    "        'for', 'he', 'know', 'will', 'but', 'with', 'so',\\\n",
    "        'they', 'now', 'well', \"'s\", 'guy', 'u', 'come',\\\n",
    "        'like', 'there', 'at', 'would', 'who', 'him',\\\n",
    "        'them', 'his', 'thing', 'where', 'should', 'an',\\\n",
    "        'please', 'maybe', 'their', 'even', 'any', 'than']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word vectors and Data splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words and the corpus have been preprocessed, now it's time to continue the setup by establishing the word vector and then splitting up the train and test sets. From there, we can tune different models and try to determine the best predictor. We first start with CountVectorizer to establish a basic bag-of-words, combined with a Naive Bayes algorithm for training.<br>\n",
    "<br>\n",
    "<b>Establishing the vector</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12333"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=sw, ngram_range=(1,1))\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "y = lines_final.Character\n",
    "\n",
    "state = 3\n",
    "\n",
    "# How many features are there?\n",
    "X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Splitting the data and training Multinomial Naive Bayes</b><br>\n",
    "Now the features, X, and the target labels, y, need to be split into test and training sets. Then we can fit the data to a Multinomial Naive Bayes model and cross validate on the training data before checking against the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "cv_scores = cross_val_score(mnb, X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:  [0.40567747 0.41510574 0.40407094 0.41031842 0.4069744 ]\n",
      "Average score:  0.4084293941887636\n"
     ]
    }
   ],
   "source": [
    "print('Scores: ', cv_scores)\n",
    "print('Average score: ', np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's only the first run, but we would certainly like to see better scores than that.<br>\n",
    "<br>\n",
    "The immediate next steps would be to try differt n-gram ranges for the count vectorizer, and different values of *alpha* for MultinomialNB. Let's experiment with n-grams first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Results for different n-gram ranges: MultinomialNB</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| ngram_range | Number of features | Mean cv score |\n",
    "|-------------|-----------------|----------------------|\n",
    "|  (1,1)      | 12,333           | 0.408                 |\n",
    "| (1,2)       | 119,582          | 0.406                 |\n",
    "| (1,3)       | 265,707          | 0.406                 |\n",
    "| (1,5)       | 511,383          | 0.404                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, increasing the n-gram range only seems to add more noise without adding any benefit. It appears it would be best to stick with a range of (1,1).<br>\n",
    "<br>\n",
    "Now we can experiment with values of *alpha* using `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha:  MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True)\n",
      "Best score:  0.4144336543498408\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'alpha': [0.1, 0.2, 0.3, 0.5, 0.75, 0.9, 1.0]}\n",
    "\n",
    "nb_grid = GridSearchCV(mnb, params, cv=5)\n",
    "\n",
    "nb_grid.fit(X_train, y_train)\n",
    "\n",
    "print('Best alpha: ', nb_grid.best_estimator_)\n",
    "print('Best score: ', nb_grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Alpha* helps, but only slightly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To try and make further improvements, we can use a weighted tf-idf vectorizer instead of a basic CountVectorizer. Stop words aren't as big of a concern here due to the inherent adjustments, but since we already have a list we may as well use it. Also, because this is a new vectorizer we will have to create new train and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_vect = TfidfVectorizer(stop_words=sw)\n",
    "\n",
    "X = tf_vect.fit_transform(corpus)\n",
    "y = lines_final.Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to run the new splits with MultinomialNB and check the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha:  MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)\n",
      "Best score:  0.40520610871579965\n"
     ]
    }
   ],
   "source": [
    "nb_grid.fit(X_train, y_train)\n",
    "\n",
    "print('Best alpha: ', nb_grid.best_estimator_)\n",
    "print('Best score: ', nb_grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that is frustrating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Random Forest</b><br>\n",
    "The results so far have been disappointing, but maybe there are other algorithms that will perform better than Naive Bayes. First, let's try Random Forest, still with the tf-idf vectors. With `oob_score` (out of bag) set to 'True', we use the out-of-bag samples as a sort of validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40653584236612\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100,\\\n",
    "                            max_features='sqrt',\\\n",
    "                            oob_score=True,\\\n",
    "                            random_state=state,\\\n",
    "                            n_jobs=-1)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "print(rf.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score is pretty in line with all the other results we've seen. There are other parameters we could play with, but the results don't look any more promising than Naive Bayes, so let's try some others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>SVM</b><br>\n",
    "We can also try Support Vector Machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C:  LinearSVC(C=0.2, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=5000,\n",
      "          multi_class='ovr', penalty='l2', random_state=3, tol=0.0001,\n",
      "          verbose=0)\n",
      "Best score:  0.42761010597574245\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm = LinearSVC(max_iter=5000, random_state=3)\n",
    "\n",
    "params = {'C': [0.15, 0.2, 0.25]}\n",
    "\n",
    "sv_grid = GridSearchCV(svm, params, cv=5)\n",
    "\n",
    "sv_grid.fit(X_train, y_train)\n",
    "\n",
    "print('Best C: ', sv_grid.best_estimator_)\n",
    "print('Best score: ', sv_grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better, but more or less the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Logistic Regression</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C:  LogisticRegression(C=2.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=5000,\n",
      "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
      "                   random_state=3, solver='newton-cg', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Best score:  0.42474916387959866\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=5000, multi_class='multinomial', random_state=3)\n",
    "\n",
    "params = {'C': [0.2, 1.0, 2.0, 5.0, 10.0],\n",
    "         'class_weight': [None],\n",
    "         'solver': ['newton-cg', 'sag']}\n",
    "\n",
    "log_grid = GridSearchCV(log_reg, params, cv=5)\n",
    "\n",
    "log_grid.fit(X_train, y_train)\n",
    "\n",
    "print('Best C: ', log_grid.best_estimator_)\n",
    "print('Best score: ', log_grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing around wih different GridSearch parameters yields some better results, but it seems clear at this point that cracking 50% is likely out of reach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_southpark",
   "language": "python",
   "name": "nlp_southpark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
