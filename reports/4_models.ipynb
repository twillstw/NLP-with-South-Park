{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now to try and predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to set up the data again. As before, refer to notebooks 1 and 2 for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the data\n",
    "lines = pd.read_csv('../data/All-seasons.csv')\n",
    "\n",
    "lines = lines[lines.Season != 'Season']\n",
    "\n",
    "lines[['Season', 'Episode']] = lines[['Season', 'Episode']].astype('int64')\n",
    "\n",
    "support_chars = ['Mr. Garrison', 'Chef', 'Sharon',\\\n",
    "                 'Mr. Mackey', 'Gerald', 'Liane', 'Sheila',\\\n",
    "                 'Stephen', 'Ms. Garrison', 'Mrs. Garrison']\n",
    "\n",
    "lines.loc[lines.Character.isin(support_chars), 'Character'] = 'Support Character'\n",
    "\n",
    "final_labels = ['Cartman', 'Stan', 'Kyle', 'Butters', 'Randy', 'Support Character']\n",
    "\n",
    "lines_final = lines[lines.Character.isin(final_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Character</th>\n",
       "      <th>Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Stan</td>\n",
       "      <td>You guys, you guys! Chef is going away. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Kyle</td>\n",
       "      <td>Going away? For how long?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Stan</td>\n",
       "      <td>Forever.\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Season  Episode Character                                        Line\n",
       "0      10        1      Stan  You guys, you guys! Chef is going away. \\n\n",
       "1      10        1      Kyle                 Going away? For how long?\\n\n",
       "2      10        1      Stan                                  Forever.\\n"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_final.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus is established here, with steps to convert everything to lowercase and remove punctuation from the end of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you guys you guys chef is going away', 'going away for how long', 'forever']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, string\n",
    "\n",
    "corpus = lines_final.Line.tolist()\n",
    "\n",
    "for line in range(len(corpus)):\n",
    "    corpus[line] = re.sub('\\\\n', '', corpus[line].rstrip()).lower()\n",
    "    corpus[line] = \" \".join(word.strip(string.punctuation) for word in corpus[line].split())\n",
    "    \n",
    "corpus[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of stop words as determined in notebook 2 needs to instatiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = ['you', 'the', 'i', 'to', 'a', 'and', 'it', 'that',\\\n",
    "              'we', 'is', 'of', 'what', 'this', 'in', 'have', 'all',\\\n",
    "              'just', 'do', 'for', \"don't\", 'are', 'be', \"it's\", 'get',\\\n",
    "              'but', 'with', 'know', 'so', 'go', 'can', 'right', 'out',\\\n",
    "              'like', 'was', 'gonna', \"that's\", 'here', 'up', 'about', \\\n",
    "              \"you're\", 'he', 'come', 'they', 'okay', 'see', 'our',\\\n",
    "              'how', 'if', 'think', 'at', 'us', \"can't\", \"we're\", 'got',\\\n",
    "              'there', 'look', 'did', 'why', 'then', 'him', 'time',\\\n",
    "              'back', 'one', 'going', 'want', 'who', \"he's\", 'from', \\\n",
    "              'some', 'his', 'will', 'need', 'make', 'take', 'yes',\\\n",
    "              \"let's\", 'because', 'them', 'has', 'as', \"what's\",\\\n",
    "              \"there's\", 'too', 'an', 'when', 'been', 'where', 'or',\\\n",
    "              'were', 'had', \"they're\", 'her', 'by', 'their', 'those',\\\n",
    "              'she', 'these', 'any', 'into', \"we've\", 'two','does',\\\n",
    "              'much', 'being', 'am', 'than']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.Series(corpus)\n",
    "y = lines_final.Character\n",
    "\n",
    "state = 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, holdout_X, y_train, holdout_y = train_test_split(X, y, test_size=0.20, stratify=y, random_state=state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a final holdout set of data, which is essentially a portion of the corpus and its corresponding character labels, and a training set of data. The corpus for the training data will be used to create the word vectors and train the models, and then the final model will be tested on the hold out set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to properly test the training model, the training data needs to be split again for validation purposes. Here I use a smaller test-size when creating the validation set to try and maximize the data used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.12, stratify=y_train, random_state=state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Establishing the CountVectorizer</b><br>\n",
    "Now the word vectors are created with CountVectorizer and are fit to the remaining training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Code_apps\\Anaconda3\\envs\\nlp_southpark\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['don', 'let', 're', 've'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(1,1), stop_words=sw)\n",
    "\n",
    "cv_train = cv.fit_transform(X_tr)\n",
    "cv_val = cv.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Using naive bayes with the vectorizer</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB(alpha=0.25)\n",
    "\n",
    "_ = nb.fit(cv_train, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy is:  0.6250250410673505\n",
      "Validation accuracy is:  0.4283196239717979\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "nb_tr_pred = nb.predict(cv_train)\n",
    "nb_val_pred = nb.predict(cv_val)\n",
    "\n",
    "print('Training accuracy is: ', accuracy_score(y_tr, nb_tr_pred))\n",
    "print('Validation accuracy is: ', accuracy_score(y_val, nb_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.42362362, 0.41389668, 0.41583166, 0.41823647, 0.41002004])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(nb, cv_train, y_tr, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Results for different n-gram ranges</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| ngram_range | Train set score | Validation set score |\n",
    "|-------------|-----------------|----------------------|\n",
    "|  (1,1)      | 0.58            | 0.42                 |\n",
    "| (1,2)       | 0.74            | 0.41                 |\n",
    "| (1,3)       | 0.78            | 0.40                 |\n",
    "| (1,5)       | 0.79            | 0.40                 |\n",
    "| (1,7)       | 0.79            | 0.399                |\n",
    "| (1,10)      | 0.79            | 0.399                |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Tf-idf</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Code_apps\\Anaconda3\\envs\\nlp_southpark\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['don', 'let', 're', 've'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tv = TfidfVectorizer(stop_words=sw)\n",
    "\n",
    "tv_train = tv.fit_transform(X_tr)\n",
    "tv_val = tv.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = nb.fit(tv_train, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy is:  0.6214191273688849\n",
      "Validation accuracy is:  0.4203877790834313\n"
     ]
    }
   ],
   "source": [
    "nb_tr_pred = nb.predict(tv_train)\n",
    "nb_val_pred = nb.predict(tv_val)\n",
    "\n",
    "print('Training accuracy is: ', accuracy_score(y_tr, nb_tr_pred))\n",
    "print('Validation accuracy is: ', accuracy_score(y_val, nb_val_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_southpark",
   "language": "python",
   "name": "nlp_southpark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
