{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now to try and predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to set up the data again. As before, refer to notebooks 1 and 2 for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Character</th>\n",
       "      <th>Line</th>\n",
       "      <th>is_cartman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Stan</td>\n",
       "      <td>You guys, you guys! Chef is going away. \\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Kyle</td>\n",
       "      <td>Going away? For how long?\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Stan</td>\n",
       "      <td>Forever.\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Season  Episode Character                                        Line  \\\n",
       "0      10        1      Stan  You guys, you guys! Chef is going away. \\n   \n",
       "1      10        1      Kyle                 Going away? For how long?\\n   \n",
       "2      10        1      Stan                                  Forever.\\n   \n",
       "\n",
       "   is_cartman  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the data\n",
    "lines = pd.read_csv('../data/All-seasons.csv')\n",
    "\n",
    "lines = lines[lines.Season != 'Season']\n",
    "\n",
    "lines[['Season', 'Episode']] = lines[['Season', 'Episode']].astype('int64')\n",
    "\n",
    "lines['is_cartman'] = 0\n",
    "\n",
    "lines.loc[lines.Character == 'Cartman', 'is_cartman'] = 1\n",
    "\n",
    "lines.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus is established here, with steps to convert everything to lowercase and remove punctuation from the end of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"what's the meaning of life why are we here\",\n",
       " \"i hope you're making the right choice\",\n",
       " \"i'm gonna miss him i'm gonna miss chef and i...and i don't know how to tell him\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, string\n",
    "\n",
    "corpus = lines.Line.tolist()\n",
    "\n",
    "for line in range(len(corpus)):\n",
    "    corpus[line] = re.sub('\\\\n', '', corpus[line].rstrip()).lower()\n",
    "    corpus[line] = \" \".join(word.strip(string.punctuation) for word in corpus[line].split())\n",
    "    \n",
    "corpus[7:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Improving the corpus by removing contractions and lemmatizing words</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a loop to expand all contractions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what is the meaning of life why are we here',\n",
       " 'i hope you are making the right choice',\n",
       " 'I am going to miss him I am going to miss chef and i...and i do not know how to tell him']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for line in range(len(corpus)):\n",
    "    corpus[line] = contractions.fix(corpus[line])\n",
    "                                  \n",
    "corpus[7:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a function to lemmatize all verbs and nouns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_lines(line):\n",
    "    word_list = word_tokenize(line)\n",
    "    \n",
    "    word_list = [lem.lemmatize(w, pos='v') for w in word_list]\n",
    "    \n",
    "    lem_line = ' '.join([lem.lemmatize(w) for w in word_list])\n",
    "    \n",
    "    return lem_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, a loop using the previous function to execute the lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what be the mean of life why be we here',\n",
       " 'i hope you be make the right choice']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for line in range(len(corpus)):\n",
    "    corpus[line] = lemmatize_lines(corpus[line])\n",
    "    \n",
    "corpus[7:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of stop words as determined in notebook 2 needs to instantiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = ['be', 'you', 'i', 'to', 'the', 'do', 'it',\\\n",
    "        'a', 'we', 'that', 'and', 'have', 'go', 'what',\\\n",
    "        'get', 'of', 'this', 'in', 'on', 'all', 'just',\\\n",
    "        'for', 'he', 'know', 'will', 'but', 'with', 'so',\\\n",
    "        'they', 'now', 'well', \"'s\", 'guy', 'u', 'come',\\\n",
    "        'like', 'there', 'at', 'would', 'who', 'him',\\\n",
    "        'them', 'his', 'thing', 'where', 'should', 'an',\\\n",
    "        'please', 'maybe', 'their', 'even', 'any', 'than']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word vectors and Data splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words and the corpus have been preprocessed, now it's time to continue the setup by establishing the word vector and then splitting up the train and test sets. From there, we can tune different models and try to determine the best predictor. We first start with CountVectorizer to establish a basic bag-of-words, combined with a Naive Bayes algorithm for training.<br>\n",
    "<br>\n",
    "<b>Establishing the vector</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "536629"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=sw, ngram_range=(1,3))\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "y = lines.is_cartman\n",
    "\n",
    "state = 3\n",
    "\n",
    "# How many features are there?\n",
    "X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Splitting the data and training Multinomial Naive Bayes</b><br>\n",
    "Now the features, X, and the target labels, y, need to be split into test and training sets. Then we can fit the data to a Multinomial Naive Bayes model and cross validate on the training data before checking against the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "cv_scores = cross_val_score(mnb, X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:  [0.86255542 0.86174929 0.86213847 0.86242693 0.8612175 ]\n",
      "Average score:  0.8620175217113915\n"
     ]
    }
   ],
   "source": [
    "print('Scores: ', cv_scores)\n",
    "print('Average score: ', np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would certainly like to see better results, but it's not terrible for the first run.<br>\n",
    "<br>\n",
    "The immediate next steps would be to try differt n-gram ranges for the count vectorizer, and different values of *alpha* for MultinomialNB. Let's experiment with n-grams first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Results for different n-gram ranges: MultinomialNB</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| ngram_range | Number of features | Mean cv score        |\n",
    "|-------------|--------------------|----------------------|\n",
    "|  (1,1)      | 20,939             | 0.856                 |\n",
    "| (1,2)       | 232,654            | 0.860                 |\n",
    "| (1,3)       | 536,629            | 0.862                 |\n",
    "| (1,5)       | 1,060,181          | 0.849                 |\n",
    "| (2,2)       | 211,715            | 0.812                 |\n",
    "| (3,3)       | 303,975            | 0.720                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the n-grams to (1,3) improves the accuracy, while also greatly increasing the features. Anything beyond that only adds features without any added benefit.<br>\n",
    "<br>\n",
    "Now we can experiment with values of *alpha* using `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha:  MultinomialNB(alpha=1.15, class_prior=None, fit_prior=True)\n",
      "Best score:  0.8629849843797238\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'alpha': [1.1, 1.15, 1.17, 1.2, 1.25, 1.3]}\n",
    "\n",
    "nb_grid = GridSearchCV(mnb, params, cv=5)\n",
    "\n",
    "nb_grid.fit(X_train, y_train)\n",
    "\n",
    "print('Best alpha: ', nb_grid.best_estimator_)\n",
    "print('Best score: ', nb_grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Alpha* helps, but only slightly.<br>\n",
    "<br>\n",
    "Let's check the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[42748,    25],\n",
       "       [ 3365,  3477]], dtype=int64)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_train, nb_grid.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96     42773\n",
      "           1       0.99      0.51      0.67      6842\n",
      "\n",
      "    accuracy                           0.93     49615\n",
      "   macro avg       0.96      0.75      0.82     49615\n",
      "weighted avg       0.94      0.93      0.92     49615\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_train, nb_grid.predict(X_train), np.unique(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results aren't bad. The predictions were labelled correctly 93% of the time, and the model was very good at correctly predicting the non-Cartman class. Cartman lines were correctly predicted about half the time.<br>\n",
    "<br>\n",
    "Now to validate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17953   379]\n",
      " [ 2612   320]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.98      0.92     18332\n",
      "           1       0.46      0.11      0.18      2932\n",
      "\n",
      "    accuracy                           0.86     21264\n",
      "   macro avg       0.67      0.54      0.55     21264\n",
      "weighted avg       0.82      0.86      0.82     21264\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, nb_grid.predict(X_test)))\n",
    "print(classification_report(y_test, nb_grid.predict(X_test), np.unique(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason, the accuracy is similar, but the f1 score for the minority label is much worse. We'll see if this trend continues with further testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Common phrases</b><br>\n",
    "We can also use the CountVectorizer along with different n-gram ranges to find common phrases used by Cartman. By using different ranges we can examine phrases of differing lengths.<br>\n",
    "<br>\n",
    "First, we use the `is_cartman` series to find the indices for all of Cartman's lines. These indices will then be used to select which arrays to pull using the `X` variable that was established earlier by fitting the corpus to the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = list(y[y ==1].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we enumerate over the arrays searching for any words that appear in that document. This won't give the actual word, but will give the indice for that word that can then be referenced in the vectorizer vocabulary to find the actual word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_values = []\n",
    "    \n",
    "for i in ind:\n",
    "    vec = X[i].toarray().tolist()\n",
    "    vec = vec[0]\n",
    "    for i, el in enumerate(vec):\n",
    "        if el > 0:\n",
    "            vocab_values.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `vocabulary_` attribute provides a dictionary of every word used in the vectorizer along with indice that corresponds with that word. These keys and values are then switched in the `new_vocab` dictionary so that words can be referenced by searching for the indice as the dictionary key. Then, the indices from the `vocab_values` list are used to create a simple list, not a set, of all the words or phrases used by Cartman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.vocabulary_\n",
    "\n",
    "new_vocab = dict([(v, k) for k, v in vocab.items()])\n",
    "\n",
    "vocab_words = []\n",
    "for i in vocab_values:\n",
    "    word = new_vocab[i]\n",
    "    vocab_words.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a simple counter can be used on the `vocab_words` list to find the most common words and phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('oh my god', 72),\n",
       " ('can not believe', 15),\n",
       " ('not tell me', 12),\n",
       " ('no no no', 12),\n",
       " ('can not wait', 10),\n",
       " ('can not let', 10),\n",
       " ('let me see', 9),\n",
       " ('dude can not', 9),\n",
       " ('no can not', 9),\n",
       " ('why can not', 9)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "cartman_phrases = Counter(vocab_words)\n",
    "cartman_phrases.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Most common words and phrases</b><br>\n",
    "<br>\n",
    "\n",
    "| Single words\\: (1,1)        | Two word phrases\\: (2,2)| Three word phrases\\: (3,3)|\n",
    "|-----------------------------|-------------------------|---------------------------|\n",
    "| not                         |  can not                |  oh my god                |\n",
    "| my                          |  my god                 | can not believe           |\n",
    "| can                         |  oh my                  | not tell me               |\n",
    "| me                          |  not want               | no no no                  |\n",
    "| oh                          |  not think              | can not wait              |\n",
    "| your                        | south park              | can not let               |\n",
    "| no                          | no not                  | let me see                |\n",
    "| yeah                        | why not                 | dude can not              |\n",
    "| here                        | tell me                 | no can not                |\n",
    "| right                       | no no                   | why can not               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entries in the table above aren't as distinct as I had hoped. All the negative words kind of get in the way but, on the other hand, it shows how central the negations are to the tone of the dialogue. The sentiment of these phrases would be much different if 'no' and 'not' were removed from the corpus. Also, the expanded contractions and other preprocessing techniques kind of dilute the phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To try and make further improvements, we can use a weighted tf-idf vectorizer instead of a basic CountVectorizer. Stop words aren't as big of a concern here due to the inherent adjustments, but since we already have a list we may as well use it. Also, because this is a new vectorizer we will have to create new train and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_vect = TfidfVectorizer(stop_words=sw, ngram_range=(1,1))\n",
    "\n",
    "X = tf_vect.fit_transform(corpus)\n",
    "y = lines.is_cartman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to run the new splits with MultinomialNB and check the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha:  MultinomialNB(alpha=1.35, class_prior=None, fit_prior=True)\n",
      "Best score:  0.8624811045046861\n"
     ]
    }
   ],
   "source": [
    "params = {'alpha': [1.0, 1.25, 1.35, 1.4]}\n",
    "\n",
    "t_nb_grid = GridSearchCV(mnb, params, cv=5)\n",
    "\n",
    "t_nb_grid.fit(X_train, y_train)\n",
    "\n",
    "print('Best alpha: ', t_nb_grid.best_estimator_)\n",
    "print('Best score: ', t_nb_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[42766,     7],\n",
       "       [ 6733,   109]], dtype=int64)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train, t_nb_grid.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.93     42773\n",
      "           1       0.94      0.02      0.03      6842\n",
      "\n",
      "    accuracy                           0.86     49615\n",
      "   macro avg       0.90      0.51      0.48     49615\n",
      "weighted avg       0.87      0.86      0.80     49615\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, t_nb_grid.predict(X_train), np.unique(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combined with MultinomialNB, the tf-idf vectorizer has about the same accuracy compared to CountVectorizer, but the confusion matrix shows a significant decrease in the predictive ability. The precision for Cartman predictions is high, but recall is terrible, lowering the f1 score to 0.03. However, there may be other algorithms better suited to the weighted vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Random Forest</b><br>\n",
    "The results so far have been less than stellar, but maybe there are other algorithms that will perform better than Naive Bayes. First, let's try Random Forest, still with the tf-idf vectors. With `oob_score` (out of bag) set to 'True', we use the out-of-bag samples as a sort of validation set. I also set `class_weight` to 'balanced' to try to account for the class imbalance. This lowers the accuracy, but should improve the other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7751284893681346\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=400,\\\n",
    "                            max_features='sqrt',\\\n",
    "                            class_weight='balanced',\\\n",
    "                            max_depth=5,\\\n",
    "                            oob_score=True,\\\n",
    "                            random_state=state,\\\n",
    "                            n_jobs=-1)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "print(rf.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[35849,  6924],\n",
       "       [ 3402,  3440]], dtype=int64)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train, rf.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.84      0.87     42773\n",
      "           1       0.33      0.50      0.40      6842\n",
      "\n",
      "    accuracy                           0.79     49615\n",
      "   macro avg       0.62      0.67      0.64     49615\n",
      "weighted avg       0.83      0.79      0.81     49615\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, rf.predict(X_train), np.unique(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The oob score is pretty in line with all the other results we've seen, but the results from the classification report far exceed anything else seen so far. The predictions for the Cartman class are still questionable, but are improved overall. Let's see if it holds up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7787810383747178"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_pred = rf.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, rf_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15211,  3121],\n",
       "       [ 1583,  1349]], dtype=int64)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, rf_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.83      0.87     18332\n",
      "           1       0.30      0.46      0.36      2932\n",
      "\n",
      "    accuracy                           0.78     21264\n",
      "   macro avg       0.60      0.64      0.62     21264\n",
      "weighted avg       0.82      0.78      0.80     21264\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, rf_pred, np.unique(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance dropped some, but it's still better than the previous ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>SVM</b><br>\n",
    "We can also try Support Vector Machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C:  LinearSVC(C=0.5, class_weight={0: 0.4, 1: 2}, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=5000,\n",
      "          multi_class='ovr', penalty='l2', random_state=3, tol=0.0001,\n",
      "          verbose=0)\n",
      "Best score:  0.7897813161342336\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm = LinearSVC(max_iter=5000, class_weight={0: 0.4, 1: 2}, random_state=3)\n",
    "\n",
    "params = {'C': [0.5, 0.6, 0.7, 0.8]}\n",
    "\n",
    "sv_grid = GridSearchCV(svm, params, cv=5)\n",
    "\n",
    "sv_grid.fit(X_train, y_train)\n",
    "\n",
    "print('Best C: ', sv_grid.best_estimator_)\n",
    "print('Best score: ', sv_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[37131,  5642],\n",
       "       [ 1765,  5077]], dtype=int64)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train, sv_grid.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some improvements with recall. Now to validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15230,  3102],\n",
       "       [ 1493,  1439]], dtype=int64)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, sv_grid.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.83      0.87     18332\n",
      "           1       0.32      0.49      0.39      2932\n",
      "\n",
      "    accuracy                           0.78     21264\n",
      "   macro avg       0.61      0.66      0.63     21264\n",
      "weighted avg       0.83      0.78      0.80     21264\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, sv_grid.predict(X_test), np.unique(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar observation as before where the accuracy is similar, but there is a significant change in the confusion matrix results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Logistic Regression</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C:  LogisticRegression(C=1.5, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=4000,\n",
      "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
      "                   random_state=3, solver='newton-cg', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Best score:  0.8678222311800867\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=4000, multi_class='multinomial', random_state=3)\n",
    "\n",
    "params = {'C': [1.0, 1.5, 1.75],\n",
    "         'solver': ['newton-cg', 'sag']}\n",
    "\n",
    "log_grid = GridSearchCV(log_reg, params, cv=5)\n",
    "\n",
    "log_grid.fit(X_train, y_train)\n",
    "\n",
    "print('Best C: ', log_grid.best_estimator_)\n",
    "print('Best score: ', log_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[42396,   377],\n",
       "       [ 5546,  1296]], dtype=int64)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train, log_grid.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing around wih different GridSearch parameters yields some better results, but there's still a similar tradeoff. The precision is higher here, but the recall is poor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Boosting</b><br>\n",
    "<br>\n",
    "Here's GradientBoosting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb = GradientBoostingClassifier(max_depth=3, n_estimators=150, learning_rate=0.05, random_state=state)\n",
    "\n",
    "cv_boost = cross_val_score(gb, X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:  [0.86648529 0.86517533 0.86375088 0.86585366 0.86655916]\n",
      "Average score:  0.8655648645006405\n"
     ]
    }
   ],
   "source": [
    "print('Scores: ', cv_boost)\n",
    "print('Average score: ', np.mean(cv_boost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[42739,    34],\n",
       "       [ 6525,   317]], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb.fit(X_train, y_train)\n",
    "\n",
    "confusion_matrix(y_train, gb.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now to validate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18297    35]\n",
      " [ 2841    91]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93     18332\n",
      "           1       0.72      0.03      0.06      2932\n",
      "\n",
      "    accuracy                           0.86     21264\n",
      "   macro avg       0.79      0.51      0.49     21264\n",
      "weighted avg       0.85      0.86      0.81     21264\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, gb.predict(X_test)))\n",
    "print(classification_report(y_test, gb.predict(X_test), np.unique(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still a lousy tradeoff between precision and recall for the minority class.<br>\n",
    "<br>\n",
    "Now AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "                   base_estimator=DecisionTreeClassifier(class_weight='balanced',\n",
       "                                                         criterion='gini',\n",
       "                                                         max_depth=1,\n",
       "                                                         max_features=None,\n",
       "                                                         max_leaf_nodes=None,\n",
       "                                                         min_impurity_decrease=0.0,\n",
       "                                                         min_impurity_split=None,\n",
       "                                                         min_samples_leaf=1,\n",
       "                                                         min_samples_split=2,\n",
       "                                                         min_weight_fraction_leaf=0.0,\n",
       "                                                         presort=False,\n",
       "                                                         random_state=3,\n",
       "                                                         splitter='best'),\n",
       "                   learning_rate=1.0, n_estimators=200, random_state=None)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_b = DecisionTreeClassifier(max_depth=1, class_weight='balanced', random_state=state)\n",
    "\n",
    "adb = AdaBoostClassifier(base_estimator=dt_b, n_estimators=200)\n",
    "\n",
    "adb.fit(X_train, y_train)\n",
    "\n",
    "#cv_ada = cross_val_score(adb, X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.7622291645671672\n"
     ]
    }
   ],
   "source": [
    "print('Score: ', accuracy_score(y_train, adb.predict(X_train)))\n",
    "#print('Average score: ', np.mean(cv_ada))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33925,  8848],\n",
       "       [ 2949,  3893]], dtype=int64)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train, adb.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14396  3936]\n",
      " [ 1424  1508]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.79      0.84     18332\n",
      "           1       0.28      0.51      0.36      2932\n",
      "\n",
      "    accuracy                           0.75     21264\n",
      "   macro avg       0.59      0.65      0.60     21264\n",
      "weighted avg       0.82      0.75      0.78     21264\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, adb.predict(X_test)))\n",
    "print(classification_report(y_test, adb.predict(X_test), np.unique(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An improvement over gradient boost, but still hasn't exceeded SVM, and the computational cost is much more severe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_southpark",
   "language": "python",
   "name": "nlp_southpark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
